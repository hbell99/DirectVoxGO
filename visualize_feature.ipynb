{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "09f95737",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using /home/hydeng/.cache/torch_extensions as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/hydeng/.cache/torch_extensions/adam_upd_cuda/build.ninja...\n",
      "Building extension module adam_upd_cuda...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module adam_upd_cuda...\n",
      "Using /home/hydeng/.cache/torch_extensions as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/hydeng/.cache/torch_extensions/render_utils_cuda/build.ninja...\n",
      "Building extension module render_utils_cuda...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module render_utils_cuda...\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import imageio\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torchvision import transforms\n",
    "\n",
    "from lib import utils, tri_dvgo_multiscene"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f686966b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_class = tri_dvgo_multiscene.DirectVoxGO\n",
    "# ckpt_path = 'logs/tri_dvgo_multiscene/nerf_synthetic/rnd_liif_posemb_conv_down1/fine_last.tar'\n",
    "# ckpt_path = 'logs/tri_dvgo_multiscene/nerf_synthetic/3conv_liif_pretrain_down4/fine_last.tar'\n",
    "# ckpt_path = 'logs/tri_dvgo_multiscene/nerf_synthetic/3conv_liif_pretrain_down4_cosine/fine_last.tar'\n",
    "ckpt_path = 'logs/tri_dvgo_multiscene/nerf_synthetic/3conv_liif_pretrain_down4_cosine_featunfold/fine_330000.tar'\n",
    "\n",
    "# ckpt_path = 'logs/tri_dvgo_multiscene/nerf_synthetic/3MLP_liif_pretrain_down4/fine_last.tar'\n",
    "# ckpt_path = 'logs/tri_dvgo_multiscene/nerf_synthetic/MLP_liif_pretrain_down4/fine_last.tar'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "098cd2b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initialized encoder networks\n",
      "{'xy': Conv_Mapping(\n",
      "  (body): Sequential(\n",
      "    (0): ResBlock(\n",
      "      (body): Sequential(\n",
      "        (0): Conv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (1): ReLU(inplace=True)\n",
      "        (2): Conv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      )\n",
      "    )\n",
      "    (1): ResBlock(\n",
      "      (body): Sequential(\n",
      "        (0): Conv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (1): ReLU(inplace=True)\n",
      "        (2): Conv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      )\n",
      "    )\n",
      "    (2): ResBlock(\n",
      "      (body): Sequential(\n",
      "        (0): Conv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (1): ReLU(inplace=True)\n",
      "        (2): Conv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      )\n",
      "    )\n",
      "    (3): ResBlock(\n",
      "      (body): Sequential(\n",
      "        (0): Conv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (1): ReLU(inplace=True)\n",
      "        (2): Conv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      )\n",
      "    )\n",
      "    (4): ResBlock(\n",
      "      (body): Sequential(\n",
      "        (0): Conv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (1): ReLU(inplace=True)\n",
      "        (2): Conv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      )\n",
      "    )\n",
      "    (5): Conv2d(80, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  )\n",
      "), 'yz': Conv_Mapping(\n",
      "  (body): Sequential(\n",
      "    (0): ResBlock(\n",
      "      (body): Sequential(\n",
      "        (0): Conv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (1): ReLU(inplace=True)\n",
      "        (2): Conv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      )\n",
      "    )\n",
      "    (1): ResBlock(\n",
      "      (body): Sequential(\n",
      "        (0): Conv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (1): ReLU(inplace=True)\n",
      "        (2): Conv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      )\n",
      "    )\n",
      "    (2): ResBlock(\n",
      "      (body): Sequential(\n",
      "        (0): Conv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (1): ReLU(inplace=True)\n",
      "        (2): Conv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      )\n",
      "    )\n",
      "    (3): ResBlock(\n",
      "      (body): Sequential(\n",
      "        (0): Conv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (1): ReLU(inplace=True)\n",
      "        (2): Conv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      )\n",
      "    )\n",
      "    (4): ResBlock(\n",
      "      (body): Sequential(\n",
      "        (0): Conv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (1): ReLU(inplace=True)\n",
      "        (2): Conv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      )\n",
      "    )\n",
      "    (5): Conv2d(80, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  )\n",
      "), 'zx': Conv_Mapping(\n",
      "  (body): Sequential(\n",
      "    (0): ResBlock(\n",
      "      (body): Sequential(\n",
      "        (0): Conv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (1): ReLU(inplace=True)\n",
      "        (2): Conv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      )\n",
      "    )\n",
      "    (1): ResBlock(\n",
      "      (body): Sequential(\n",
      "        (0): Conv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (1): ReLU(inplace=True)\n",
      "        (2): Conv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      )\n",
      "    )\n",
      "    (2): ResBlock(\n",
      "      (body): Sequential(\n",
      "        (0): Conv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (1): ReLU(inplace=True)\n",
      "        (2): Conv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      )\n",
      "    )\n",
      "    (3): ResBlock(\n",
      "      (body): Sequential(\n",
      "        (0): Conv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (1): ReLU(inplace=True)\n",
      "        (2): Conv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      )\n",
      "    )\n",
      "    (4): ResBlock(\n",
      "      (body): Sequential(\n",
      "        (0): Conv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (1): ReLU(inplace=True)\n",
      "        (2): Conv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      )\n",
      "    )\n",
      "    (5): Conv2d(80, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  )\n",
      ")}\n",
      "initialized mapping networks\n",
      "dvgo: set density bias shift to -4.595119850134584\n",
      "\n",
      "dvgo: voxel_size       4096000\n",
      "dvgo: voxel_size       tensor(0.0376, device='cuda:0')\n",
      "dvgo: world_size       tensor([168, 168, 144], device='cuda:0')\n",
      "dvgo: voxel_size_base  tensor(0.0376, device='cuda:0')\n",
      "dvgo: voxel_size_ratio tensor(1., device='cuda:0')\n",
      "\n",
      "\u001b[96mimplicit voxel feat!!!\u001b[0m\n",
      "loading pretrained state dict from liif\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/home/hydeng/Documents/SR_NeRF/code/DirectVoxGO/pretrained/edsr-baseline-liif.pth'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [3]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mutils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_class\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/data/hydeng/SR_NeRF/DirectVoxGO/lib/utils.py:67\u001b[0m, in \u001b[0;36mload_model\u001b[0;34m(model_class, ckpt_path)\u001b[0m\n\u001b[1;32m     65\u001b[0m ckpt \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(ckpt_path)\n\u001b[1;32m     66\u001b[0m ckpt[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel_kwargs\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mload_liif_sd\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m---> 67\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mmodel_class\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mckpt\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmodel_kwargs\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     68\u001b[0m model\u001b[38;5;241m.\u001b[39mload_state_dict(ckpt[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel_state_dict\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloaded checkpoints!!!\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m/data/hydeng/SR_NeRF/DirectVoxGO/lib/tri_dvgo_multiscene.py:260\u001b[0m, in \u001b[0;36mDirectVoxGO.__init__\u001b[0;34m(self, xyz_min, xyz_max, num_voxels, num_voxels_base, alpha_init, mask_cache_path, mask_cache_thres, fast_color_thres, rgbnet_dim, rgbnet_direct, rgbnet_full_implicit, rgbnet_depth, rgbnet_width, viewbase_pe, interp_width, interp_depth, tri_aggregation, feat_pe, feat_fourier, map_depth, map_width, liif, no_voxel_feat, posbase_pe, global_cell_decode, implicit_voxel_feat, feat_unfold, local_ensemble, cell_decode, cat_posemb, n_scene, mlp_map, conv_map, closed_map, compute_consistency, n_mapping, compute_cosine, use_anchor_liif, use_siren, name, n_feats, n_resblocks, res_scale, scale, no_upsampling, rgb_range, pretrained_state_dict, liif_state_dict, load_liif_sd, **kwargs)\u001b[0m\n\u001b[1;32m    258\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m load_liif_sd \u001b[38;5;129;01mor\u001b[39;00m use_anchor_liif:\n\u001b[1;32m    259\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloading pretrained state dict from liif\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 260\u001b[0m     liif_sd \u001b[38;5;241m=\u001b[39m \u001b[43mload_liif_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mliif_state_dict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    261\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minterp_xy \u001b[38;5;241m=\u001b[39m upadate_interp_state_dict(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minterp_xy, liif_sd)\n\u001b[1;32m    262\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minterp_yz \u001b[38;5;241m=\u001b[39m upadate_interp_state_dict(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minterp_yz, liif_sd)\n",
      "File \u001b[0;32m/data/hydeng/SR_NeRF/DirectVoxGO/lib/tri_dvgo_multiscene.py:36\u001b[0m, in \u001b[0;36mload_liif_state_dict\u001b[0;34m(liif_path)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_liif_state_dict\u001b[39m(liif_path):\n\u001b[0;32m---> 36\u001b[0m     liif_model_sd \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mliif_path\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msd\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     38\u001b[0m     liif_sd \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m     39\u001b[0m     liif_sd[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel.0.weight\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m liif_model_sd[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimnet.layers.0.weight\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[0;32m~/.conda/envs/nsvf/lib/python3.8/site-packages/torch/serialization.py:594\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    591\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m pickle_load_args\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m    592\u001b[0m     pickle_load_args[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m--> 594\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_file_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_file:\n\u001b[1;32m    595\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[1;32m    596\u001b[0m         \u001b[38;5;66;03m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[1;32m    597\u001b[0m         \u001b[38;5;66;03m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[1;32m    598\u001b[0m         \u001b[38;5;66;03m# reset back to the original position.\u001b[39;00m\n\u001b[1;32m    599\u001b[0m         orig_position \u001b[38;5;241m=\u001b[39m opened_file\u001b[38;5;241m.\u001b[39mtell()\n",
      "File \u001b[0;32m~/.conda/envs/nsvf/lib/python3.8/site-packages/torch/serialization.py:230\u001b[0m, in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_open_file_like\u001b[39m(name_or_buffer, mode):\n\u001b[1;32m    229\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[0;32m--> 230\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_open_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    231\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    232\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m mode:\n",
      "File \u001b[0;32m~/.conda/envs/nsvf/lib/python3.8/site-packages/torch/serialization.py:211\u001b[0m, in \u001b[0;36m_open_file.__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, mode):\n\u001b[0;32m--> 211\u001b[0m     \u001b[38;5;28msuper\u001b[39m(_open_file, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/home/hydeng/Documents/SR_NeRF/code/DirectVoxGO/pretrained/edsr-baseline-liif.pth'"
     ]
    }
   ],
   "source": [
    "model = utils.load_model(model_class, ckpt_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f4d3145",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_image(path):\n",
    "    image = imageio.imread(path)\n",
    "    image = (np.array(image) / 255.).astype(np.float32)\n",
    "    if image.shape[-1] == 4:\n",
    "        image = image[...,:3]*image[...,-1:] + (1.-image[...,-1:])\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfed673c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_poses(path, idxs):\n",
    "    with open(path, 'r') as f:\n",
    "        meta = json.load(f)\n",
    "    poses = []\n",
    "    for id in idxs:\n",
    "        frame = meta['frames'][id]\n",
    "        poses.append(np.array(frame['transform_matrix']))\n",
    "    poses = np.stack(poses)\n",
    "    return poses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3182ed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "imagedir = 'data/nerf_synthetic/mic/train'\n",
    "json_path = 'data/nerf_synthetic/mic/transforms_train.json'\n",
    "idxs = [34, 49, 63]\n",
    "\n",
    "images = []\n",
    "for id in idxs:\n",
    "    path = os.path.join(imagedir, f'r_{id}.png')\n",
    "    images.append(read_image(path))\n",
    "images = np.stack(images)\n",
    "\n",
    "poses = read_poses(json_path, idxs)\n",
    "\n",
    "images = torch.FloatTensor(images)\n",
    "poses = torch.FloatTensor(poses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba9c35ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(images.shape, poses.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3ea1854",
   "metadata": {},
   "outputs": [],
   "source": [
    "images = images.permute(0, 3, 1, 2)\n",
    "h, w = images.shape[-2:]\n",
    "h, w = h // 4, w // 4\n",
    "resize = transforms.Resize([h, w])\n",
    "images = resize(images)\n",
    "images = (images - 0.5) / 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3476d95",
   "metadata": {},
   "outputs": [],
   "source": [
    "feats, _, _, = model.encode_feat(images, poses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cdcaba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "394ef6fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "splits = ['xy', 'yz', 'zx']\n",
    "channels = [15, 20]\n",
    "fig, axes = plt.subplots(nrows=2, ncols=3, figsize=(12,8))\n",
    "plt.subplots_adjust(left=None,bottom=None,right=None,top=None,wspace=0.3, hspace=0)\n",
    "vmin, vmax = 1e6, -1e6\n",
    "for i, s in enumerate(splits):\n",
    "    feats[s] = (feats[s] - feats[s].min()) / (feats[s].max() - feats[s].min())\n",
    "    \n",
    "    vmin = min(feats[s].min().item(), vmin)\n",
    "    vmax = max(feats[s].max().item(), vmax)\n",
    "\n",
    "for i, channel in enumerate(channels):\n",
    "    for j, s in enumerate(splits):\n",
    "        axes[i, j].imshow(feats[s][0][channel].detach().numpy(), cmap='RdBu')\n",
    "        axes[i, j].xaxis.set_tick_params(labelsize=8)\n",
    "        axes[i, j].yaxis.set_tick_params(labelsize=8)\n",
    "\n",
    "rows = [f'channel {i}' for i in channels]\n",
    "cols = splits\n",
    "for ax, row in zip(axes[:,0], rows):\n",
    "    ax.set_ylabel(row, rotation=90, fontsize=10)\n",
    "\n",
    "for ax, col in zip(axes[0], cols):\n",
    "    ax.set_title(col, fontsize=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5615856",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
